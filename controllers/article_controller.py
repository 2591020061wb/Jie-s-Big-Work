from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity
from datetime import datetime, timedelta
from models.models import AIArticles, UserArticleViews, RiskAssessments, HealthMetrics
from extensions import db  # âœ… ä¿®å¤å¾ªç¯å¯¼å…¥
import json
import random
import traceback

# åˆ›å»ºè“å›¾
article_bp = Blueprint('article', __name__)

# ==================== è·¯ç”±å®šä¹‰ ====================

@article_bp.route('/recommended', methods=['GET'])
@jwt_required()
def get_recommended_articles():
    """è·å–ä¸ªæ€§åŒ–æ¨èæ–‡ç« ï¼ˆTop3ï¼‰"""
    try:
        user_id = get_jwt_identity()
        
        # è·å–æ‰€æœ‰æ–‡ç« 
        all_articles = AIArticles.query.order_by(AIArticles.published_at.desc()).all()
        print(f"âœ… æ•°æ®åº“æ–‡ç« æ•°é‡: {len(all_articles)}")
        
        # å¦‚æœæ–‡ç« å¤ªå°‘ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡ç« 
        if len(all_articles) < 5:
            create_sample_articles()
            all_articles = AIArticles.query.order_by(AIArticles.published_at.desc()).all()
            
        # è·å–ç”¨æˆ·å·²æŸ¥çœ‹æ–‡ç« 
        viewed_articles = UserArticleViews.query.filter_by(user_id=user_id).all()
        viewed_ids = [v.article_id for v in viewed_articles]
        
        # è·å–ç”¨æˆ·å¥åº·æ•°æ®
        risks = RiskAssessments.query.filter_by(user_id=user_id).order_by(
            RiskAssessments.assessment_date.desc()).all()
        metrics = HealthMetrics.query.filter_by(user_id=user_id).order_by(
            HealthMetrics.recorded_at.desc()).limit(5).all()
            
        # ä¸ªæ€§åŒ–æ’å
        ranked_articles = rank_articles_for_user(all_articles, viewed_ids, risks, metrics)
        top_articles = ranked_articles[:3]
        
        # æ ¼å¼åŒ–ç»“æœ
        result = []
        for article in top_articles:
            time_text = format_relative_time(article.published_at)
            result.append({
                'id': article.article_id,
                'title': article.title,
                'source': article.source or 'MedGPT Lab',
                'time': time_text
            })
            
        print(f"ğŸ“¤ åç«¯è¿”å›æ–‡ç« : {result}")
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ è·å–æ¨èå¤±è´¥: {e}\n{traceback.format_exc()}")
        return jsonify([]), 200  # è¿”å›ç©ºæ•°ç»„é¿å…å‰ç«¯500

@article_bp.route('/view', methods=['POST'])
@jwt_required()
def record_article_view():
    """è®°å½•ç”¨æˆ·æ–‡ç« æŸ¥çœ‹è¡Œä¸º"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json()
        article_id = data.get('article_id')
        
        if not article_id:
            return jsonify({"error": "ç¼ºå°‘æ–‡ç« ID"}), 400
            
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨
        article = AIArticles.query.get(article_id)
        if not article:
            return jsonify({"error": "æ–‡ç« ä¸å­˜åœ¨"}), 404
            
        # è®°å½•æŸ¥çœ‹ï¼ˆé¿å…é‡å¤ï¼‰
        existing_view = UserArticleViews.query.filter_by(
            user_id=user_id, article_id=article_id
        ).first()
        
        if existing_view:
            # æ›´æ–°æŸ¥çœ‹æ—¶é—´
            existing_view.viewed_at = datetime.now()
        else:
            # åˆ›å»ºæ–°è®°å½•
            view = UserArticleViews(
                user_id=user_id,
                article_id=article_id,
                viewed_at=datetime.now(),
                engagement_score=data.get('engagement_score', 1.0)
            )
            db.session.add(view)
            
        db.session.commit()
        return jsonify({"status": "success", "message": "å·²è®°å½•"})
        
    except Exception as e:
        db.session.rollback()
        print(f"âŒ è®°å½•æŸ¥çœ‹å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500

@article_bp.route('/detail/<int:article_id>', methods=['GET'])
@jwt_required()
def get_article_detail(article_id):
    """è·å–æ–‡ç« è¯¦æƒ…"""
    try:
        article = AIArticles.query.get(article_id)
        if not article:
            return jsonify({"error": "æ–‡ç« ä¸å­˜åœ¨"}), 404
            
        # è§£ææ ‡ç­¾ï¼ˆå®¹é”™JSONï¼‰
        tags = safe_json_loads(article.tags, [])
        
        result = {
            'id': article.article_id,
            'title': article.title,
            'summary': article.summary,
            'content': generate_article_content(article),
            'source': article.source,
            'published_at': article.published_at.strftime('%Y-%m-%d %H:%M') if article.published_at else None,
            'tags': tags
        }
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500

# ==================== è¾…åŠ©å‡½æ•° ====================

def format_relative_time(published_at):
    """æ ¼å¼åŒ–ç›¸å¯¹æ—¶é—´"""
    if not published_at:
        return "æœªçŸ¥æ—¶é—´"
        
    now = datetime.now()
    delta = now - published_at
    
    if delta < timedelta(hours=1):
        return f"{int(delta.total_seconds() // 60)}åˆ†é’Ÿå‰"
    elif delta < timedelta(hours=24):
        return f"{int(delta.total_seconds() // 3600)}å°æ—¶å‰"
    elif delta < timedelta(days=7):
        return f"{delta.days}å¤©å‰"
    else:
        return published_at.strftime('%Y-%m-%d')

def safe_json_loads(data, default=None):
    """å®‰å…¨çš„JSONè§£æ"""
    if not data:
        return default or []
        
    try:
        if isinstance(data, str):
            return json.loads(data)
        elif isinstance(data, (list, dict)):
            return data
    except (json.JSONDecodeError, TypeError):
        pass
        
    return default or []

def create_sample_articles():
    """åˆ›å»ºç¤ºä¾‹æ–‡ç« ï¼ˆä¿®å¤JSONæ ¼å¼ï¼‰"""
    sample_articles = [
        {
            'title': 'AIè¾…åŠ©ç¡çœ åˆ†æœŸè¯†åˆ«å‡†ç¡®ç‡çªç ´95%',
            'summary': 'æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ç¡çœ åˆ†æœŸç®—æ³•å¯ä»¥å‡†ç¡®è¯†åˆ«å„ä¸ªç¡çœ é˜¶æ®µã€‚',
            'tags': ['AI', 'ç¡çœ åŒ»å­¦', 'æ·±åº¦å­¦ä¹ '],
            'source': 'MedGPT Lab',
            'published_at': datetime.now() - timedelta(hours=1),
            'relevance_vector': {'sleep': 0.9, 'technology': 0.8}
        },
        {
            'title': 'å¤šæ¨¡æ€å¯ç©¿æˆ´æ•°æ®é¢„æµ‹è¡€å‹æ–°ç®—æ³•',
            'summary': 'ç ”ç©¶äººå‘˜å¼€å‘å‡ºä¸€ç§ç»“åˆå¤šç§ç”Ÿç‰©ä¿¡å·çš„ç®—æ³•ï¼Œå¯æ— åˆ›è¿ç»­ç›‘æµ‹è¡€å‹ã€‚',
            'tags': ['å¯ç©¿æˆ´è®¾å¤‡', 'è¡€å‹ç›‘æµ‹', 'ç®—æ³•'],
            'source': 'BioChrono',
            'published_at': datetime.now() - timedelta(hours=3),
            'relevance_vector': {'blood_pressure': 0.9, 'wearable': 0.8}
        },
        {
            'title': 'ä¸ªæ€§åŒ–è¿åŠ¨å¤„æ–¹çš„é—­ç¯è°ƒä¼˜æ¡ˆä¾‹',
            'summary': 'æ–°ç ”ç©¶å±•ç¤ºå¦‚ä½•åˆ©ç”¨å®æ—¶ç”Ÿç†æ•°æ®è°ƒæ•´è¿åŠ¨å¤„æ–¹ã€‚',
            'tags': ['è¿åŠ¨ç§‘å­¦', 'ä¸ªæ€§åŒ–', 'å¥åº·'],
            'source': 'SportsAI',
            'published_at': datetime.now() - timedelta(days=1),
            'relevance_vector': {'exercise': 0.9, 'personalization': 0.8}
        },
        {
            'title': 'æ™ºèƒ½ç¡çœ å‘¼å¸ç›‘æµ‹æŠ€æœ¯è¿›å±•',
            'summary': 'æ–°å‹æ™ºèƒ½ç›‘æµ‹è®¾å¤‡å¯åœ¨å®¶åº­ç¯å¢ƒä¸‹æ£€æµ‹ç¡çœ å‘¼å¸æš‚åœã€‚',
            'tags': ['ç¡çœ å‘¼å¸æš‚åœ', 'æ™ºèƒ½ç›‘æµ‹'],
            'source': 'SleepTech',
            'published_at': datetime.now() - timedelta(days=2),
            'relevance_vector': {'sleep_apnea': 0.9, 'monitoring': 0.8}
        },
        {
            'title': 'åœ°ä¸­æµ·é¥®é£Ÿå¯¹å¿ƒè¡€ç®¡å¥åº·çš„é•¿æœŸå½±å“',
            'summary': 'ä¸ºæœŸ10å¹´çš„ç ”ç©¶è¯å®ï¼Œåœ°ä¸­æµ·é¥®é£Ÿå¯æ˜¾è‘—é™ä½å¿ƒè¡€ç®¡ç–¾ç—…é£é™©ã€‚',
            'tags': ['è¥å…»', 'å¿ƒè¡€ç®¡å¥åº·'],
            'source': 'Nutrition Science',
            'published_at': datetime.now() - timedelta(days=3),
            'relevance_vector': {'nutrition': 0.9, 'cardiovascular': 0.8}
        }
    ]
    
    try:
        for data in sample_articles:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if AIArticles.query.filter_by(title=data['title']).first():
                continue
                
            article = AIArticles(
                title=data['title'],
                summary=data['summary'],
                tags=json.dumps(data['tags'], ensure_ascii=False),  # âœ… å¼ºåˆ¶JSONåºåˆ—åŒ–
                source=data['source'],
                published_at=data['published_at'],
                relevance_vector=json.dumps(data['relevance_vector'], ensure_ascii=False),
                created_at=datetime.now()
            )
            db.session.add(article)
            
        db.session.commit()
        print("âœ… ç¤ºä¾‹æ–‡ç« åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        db.session.rollback()
        print(f"âŒ åˆ›å»ºå¤±è´¥: {e}")

def rank_articles_for_user(articles, viewed_ids, risks, metrics):
    """ä¸ºç”¨æˆ·ä¸ªæ€§åŒ–æ’åºæ–‡ç« """
    # æå–å¥åº·å…³æ³¨ç‚¹
    concerns = set()
    
    # ä»é£é™©è¯„ä¼°æå–
    for risk in risks:
        if risk.risk_level in ['medium', 'high']:
            if 'é«˜è¡€å‹' in risk.disease:
                concerns.update(['blood_pressure', 'cardiovascular'])
            elif 'ç¡çœ ' in risk.disease:
                concerns.update(['sleep', 'respiratory'])
            elif 'ä»£è°¢' in risk.disease:
                concerns.update(['metabolism', 'diabetes'])
                
    # ä»å¥åº·æŒ‡æ ‡æå–
    if metrics:
        # è¡€å‹å¼‚å¸¸
        systolic_vals = [m.blood_pressure_systolic for m in metrics if m.blood_pressure_systolic]
        if systolic_vals and sum(v > 130 for v in systolic_vals) / len(systolic_vals) >= 0.5:
            concerns.add('blood_pressure')
            
        # ç¡çœ ä¸è¶³
        sleep_vals = [safe_float(m.sleep_duration) for m in metrics if m.sleep_duration]
        if sleep_vals and sum(v < 6.5 for v in sleep_vals) / len(sleep_vals) >= 0.5:
            concerns.add('sleep')
            
        # é«˜å‹åŠ›
        stress_vals = [m.stress_level for m in metrics if m.stress_level]
        if stress_vals and sum(v > 60 for v in stress_vals) / len(stress_vals) >= 0.5:
            concerns.add('stress')
            
    # é»˜è®¤å…³æ³¨ç‚¹
    if not concerns:
        concerns = {'health', 'wellness'}
        
    # å¯¹æ–‡ç« è¯„åˆ†
    scored = []
    for article in articles:
        # å·²æŸ¥çœ‹æƒ©ç½š
        view_penalty = 0.5 if article.article_id in viewed_ids else 1.0
        
        # ç›¸å…³æ€§åŒ¹é…
        relevance = safe_json_loads(article.relevance_vector, {})
        match_score = sum(relevance.get(c, 0) for c in concerns)
        
        # æ ‡ç­¾åŒ¹é…
        tags = safe_json_loads(article.tags, [])
        for tag in tags:
            for c in concerns:
                if c.lower() in tag.lower():
                    match_score += 0.3
                    
        # æ—¶é—´æ–°é²œåº¦
        recency = 1.0
        if article.published_at:
            days_old = (datetime.now() - article.published_at).days
            recency = max(0.5, 1.0 - days_old / 14)
            
        # æ€»åˆ†
        total = (match_score * 0.7 + recency * 0.3) * view_penalty
        scored.append((article, total))
        
    # æ’åº
    return [a for a, s in sorted(scored, key=lambda x: x[1], reverse=True)]

def safe_float(value, default=0.0):
    """å®‰å…¨çš„æµ®ç‚¹æ•°è½¬æ¢"""
    try:
        return float(value)
    except (ValueError, TypeError):
        return default

def generate_article_content(article):
    """åŸºäºæ‘˜è¦ç”Ÿæˆæ–‡ç« å†…å®¹"""
    if not article.summary:
        return "<p>æ–‡ç« å†…å®¹æœªèƒ½åŠ è½½ã€‚</p>"
        
    paragraphs = [
        f"<h3>{article.title}</h3>",
        f"<p><strong>æ‘˜è¦ï¼š</strong>{article.summary}</p>",
        "<h4>èƒŒæ™¯</h4>",
        "<p>åœ¨å¥åº·ç›‘æµ‹å’ŒåŒ»ç–—æŠ€æœ¯å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæ–°å‹æŠ€æœ¯å’Œæ–¹æ³•ä¸æ–­æ¶Œç°ã€‚</p>",
        "<h4>ä¸»è¦å‘ç°</h4>",
        f"<p>{article.summary}</p>",
        "<h4>å®é™…åº”ç”¨</h4>",
        "<ul>",
        "<li>æé«˜å¥åº·ç›‘æµ‹çš„å‡†ç¡®æ€§å’Œä¾¿æ·æ€§</li>",
        "<li>ä¸ºä¸ªæ€§åŒ–å¥åº·ç®¡ç†æä¾›æ•°æ®æ”¯æŒ</li>",
        "<li>å¸®åŠ©åˆ¶å®šç§‘å­¦çš„å¹²é¢„æ–¹æ¡ˆ</li>",
        "</ul>"
    ]
    
    # æ·»åŠ æ ‡ç­¾
    tags = safe_json_loads(article.tags, [])
    if tags:
        tag_html = ", ".join([f"<span class='tag'>{t}</span>" for t in tags])
        paragraphs.append(f"<p><strong>æ ‡ç­¾ï¼š</strong>{tag_html}</p>")
        
    # æ·»åŠ æ¥æºå’Œæ—¥æœŸ
    source_info = []
    if article.source:
        source_info.append(f"æ¥æº: {article.source}")
    if article.published_at:
        source_info.append(f"å‘å¸ƒäº: {article.published_at.strftime('%Y-%m-%d')}")
    if source_info:
        paragraphs.append(f"<p><em>{' | '.join(source_info)}</em></p>")
        
    return "\n".join(paragraphs)
