"""
================================================================================
åŒ»ç–—ç–¾ç—…é¢„æµ‹ç³»ç»Ÿ - BERTåŒå¡”ç¼–ç å™¨è®­ç»ƒè„šæœ¬
================================================================================
åŠŸèƒ½æ¨¡å—ï¼š
1. å¯¹æ¯”å­¦ä¹ è®­ç»ƒåŒå¡”ç¼–ç å™¨ï¼ˆQuery-DocumentåŒ¹é…ï¼‰
2. æ„å»ºç–¾ç—…ç´¢å¼•ï¼ˆåŒ…å«è¯­ä¹‰å‘é‡å’ŒIDFæƒé‡ï¼‰
3. æ··åˆæ£€ç´¢ï¼ˆBERTè¯­ä¹‰ç›¸ä¼¼åº¦ + è¯é¢é‡å ï¼‰

æŠ€æœ¯æ ˆï¼š
- PyTorch + Transformersï¼ˆBERTæ¨¡å‹ï¼‰
- å¯¹æ¯”å­¦ä¹ ï¼ˆInfoNCEæŸå¤±ï¼‰
- å¤šå­—æ®µç»“æ„åŒ–è¡¨ç¤ºï¼ˆç—‡çŠ¶/æè¿°/ç—…å› /ç§‘å®¤ï¼‰
- IDFåŠ æƒè¯é¢åŒ¹é…

ä½œè€…ï¼šYour Name
æ—¥æœŸï¼š2024-01-XX
ç‰ˆæœ¬ï¼šv3.0ï¼ˆå­—æ®µæ ‡è®° + IDFåŠ æƒ + æ··åˆæ£€ç´¢ï¼‰
================================================================================
"""

# ==================== ç¬¬1éƒ¨åˆ†ï¼šä¾èµ–å¯¼å…¥ ====================
import os  # æ–‡ä»¶è·¯å¾„æ“ä½œ
import ast  # å®‰å…¨è§£æPythonå­—é¢é‡ï¼ˆå¦‚å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼‰
import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import json  # JSONæ•°æ®å¤„ç†
import random  # éšæœºæ•°ç”Ÿæˆï¼ˆç”¨äºæ•°æ®é›†åˆ’åˆ†ï¼‰
import numpy as np  # æ•°å€¼è®¡ç®—ï¼ˆå‘é‡æ“ä½œï¼‰
import pandas as pd  # æ•°æ®è¡¨æ ¼å¤„ç†ï¼ˆè¯»å–CSVï¼‰
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from torch.utils.data import Dataset, DataLoader  # æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
from torch.optim import AdamW  # AdamWä¼˜åŒ–å™¨ï¼ˆå¸¦æƒé‡è¡°å‡çš„Adamï¼‰
from transformers import (
    BertTokenizer,  # BERTåˆ†è¯å™¨
    BertModel,  # BERTé¢„è®­ç»ƒæ¨¡å‹
    get_linear_schedule_with_warmup  # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆçº¿æ€§é¢„çƒ­+çº¿æ€§è¡°å‡ï¼‰
)
from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º
from math import log  # æ•°å­¦å¯¹æ•°å‡½æ•°ï¼ˆè®¡ç®—IDFï¼‰

# ==================== ç¬¬2éƒ¨åˆ†ï¼šè·¯å¾„å’Œè¶…å‚æ•°é…ç½® ====================

# ===== è·¯å¾„é…ç½® =====
MODEL_PATH = r'C:\bert-base-chinese'  # BERTé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
DATA_PATH = r'C:\Users\Gustav  Adolf\Music\åŸºäºpythonåŒ»ç–—ç–¾ç—…æ•°æ®åˆ†æå¤§å±å¯è§†åŒ–ç³»ç»Ÿ\medical.csv'  # åŒ»ç–—æ•°æ®CSVæ–‡ä»¶è·¯å¾„
OUT_DIR = r'C:\medical_biencoder'  # è¾“å‡ºç›®å½•ï¼ˆä¿å­˜è®­ç»ƒåçš„æ¨¡å‹å’Œç´¢å¼•ï¼‰
INDEX_PATH = os.path.join(OUT_DIR, 'biencoder_index.npz')  # ç–¾ç—…ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆNumPyå‹ç¼©æ ¼å¼ï¼‰

# ===== è®­ç»ƒè¶…å‚æ•° =====
MAX_LEN = 128  # BERTè¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆè¶…è¿‡ä¼šæˆªæ–­ï¼‰
BATCH_SIZE = 16  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ¬¡å‰å‘ä¼ æ’­çš„æ ·æœ¬æ•°ï¼‰
EPOCHS = 1  # è®­ç»ƒè½®æ•°ï¼ˆéå†æ•´ä¸ªæ•°æ®é›†çš„æ¬¡æ•°ï¼‰
LR = 2e-5  # å­¦ä¹ ç‡ï¼ˆAdamWä¼˜åŒ–å™¨çš„åˆå§‹å­¦ä¹ ç‡ï¼‰
WARMUP = 0.1  # é¢„çƒ­æ­¥æ•°æ¯”ä¾‹ï¼ˆæ€»æ­¥æ•°çš„10%ç”¨äºçº¿æ€§é¢„çƒ­ï¼‰
TEMP = 0.05  # å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ï¼‰
SEED = 42  # éšæœºç§å­ï¼ˆç¡®ä¿å®éªŒå¯å¤ç°ï¼‰

# ===== å­—æ®µæ ‡è®°ï¼ˆç»“æ„åŒ–è¡¨ç¤ºï¼‰ =====
# ç”¨äºåŒºåˆ†æ–‡æœ¬ä¸­çš„ä¸åŒå­—æ®µï¼ˆç—‡çŠ¶/æè¿°/ç—…å› /ç§‘å®¤ï¼‰
# ç¤ºä¾‹ï¼š"[SYM] å¤´ç—› å‘çƒ­ [SEP] [DESC] å¸¸è§æ„Ÿå†’ç—‡çŠ¶ [SEP] [CAT] å‘¼å¸å†…ç§‘"
FIELD_TAGS = ['[SYM]', '[DESC]', '[CAUSE]', '[CAT]']

# ===== åˆå§‹åŒ– =====
os.makedirs(OUT_DIR, exist_ok=True)  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
random.seed(SEED)  # è®¾ç½®Pythonéšæœºç§å­
np.random.seed(SEED)  # è®¾ç½®NumPyéšæœºç§å­
torch.manual_seed(SEED)  # è®¾ç½®PyTorchéšæœºç§å­

# ==================== ç¬¬3éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†å‡½æ•° ====================

def parse_list_str(x):
    """
    å®‰å…¨è§£æå­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
    
    å‚æ•°ï¼š
        x: å¯èƒ½çš„è¾“å…¥ç±»å‹
            - None/NaN: è¿”å›ç©ºåˆ—è¡¨
            - "['å¤´ç—›', 'å‘çƒ­']": è§£æä¸ºPythonåˆ—è¡¨
            - "å¤´ç—›": è¿”å›å•å…ƒç´ åˆ—è¡¨
    
    è¿”å›ï¼š
        list[str]: å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆç©ºåˆ—è¡¨æˆ–åŒ…å«å…ƒç´ ï¼‰
    
    ç¤ºä¾‹ï¼š
        >>> parse_list_str("['å¤´ç—›', 'å‘çƒ­']")
        ['å¤´ç—›', 'å‘çƒ­']
        
        >>> parse_list_str("å¤´ç—›")
        ['å¤´ç—›']
        
        >>> parse_list_str(None)
        []
    """
    # 1. å¤„ç†Noneå€¼
    if x is None:
        return []
    
    # 2. å¤„ç†NaNå€¼ï¼ˆpandasçš„ç¼ºå¤±å€¼ï¼‰
    if isinstance(x, float) and np.isnan(x):
        return []
    
    # 3. è½¬ä¸ºå­—ç¬¦ä¸²
    s = str(x)
    
    try:
        # 4. å°è¯•ç”¨ast.literal_evalå®‰å…¨è§£æï¼ˆé¿å…evalçš„å®‰å…¨é£é™©ï¼‰
        # è¾“å…¥ï¼š"['å¤´ç—›', 'å‘çƒ­']" â†’ è¾“å‡ºï¼š['å¤´ç—›', 'å‘çƒ­']
        v = ast.literal_eval(s)
        if isinstance(v, list):
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            return [str(i).strip() for i in v if str(i).strip()]
    except Exception:
        # è§£æå¤±è´¥ï¼ˆå¦‚æ™®é€šå­—ç¬¦ä¸²"å¤´ç—›"ï¼‰
        pass
    
    # 5. ä½œä¸ºå•ä¸ªå­—ç¬¦ä¸²å¤„ç†
    return [s.strip()] if s.strip() else []

def format_query(symps):
    """
    æ ¼å¼åŒ–ç”¨æˆ·æŸ¥è¯¢ï¼ˆæ·»åŠ å­—æ®µæ ‡è®°ï¼‰
    
    å‚æ•°ï¼š
        symps (list[str]): ç—‡çŠ¶åˆ—è¡¨ï¼Œå¦‚ ['å¤´ç—›', 'å‘çƒ­']
    
    è¿”å›ï¼š
        str: æ ¼å¼åŒ–åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œå¦‚ "[SYM] å¤´ç—› å‘çƒ­"
    
    ç¤ºä¾‹ï¼š
        >>> format_query(['å¤´ç—›', 'å‘çƒ­'])
        '[SYM] å¤´ç—› å‘çƒ­'
    """
    # 1. è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    symps = [t for t in symps if t]
    
    # 2. å¦‚æœæ²¡æœ‰ç—‡çŠ¶ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    if not symps:
        return ''
    
    # 3. æ·»åŠ ç—‡çŠ¶å­—æ®µæ ‡è®°
    return f"{FIELD_TAGS[0]} " + ' '.join(symps)

def format_doc(symps, desc, cause, cat):
    """
    æ ¼å¼åŒ–æ–‡æ¡£ï¼ˆå¤šå­—æ®µç»“æ„åŒ–è¡¨ç¤ºï¼‰
    
    å‚æ•°ï¼š
        symps (list[str]): ç—‡çŠ¶åˆ—è¡¨
        desc (str): ç–¾ç—…æè¿°
        cause (str): ç—…å› 
        cat (str): ç§‘å®¤åˆ†ç±»
    
    è¿”å›ï¼š
        str: æ ¼å¼åŒ–åçš„æ–‡æ¡£æ–‡æœ¬
    
    æ ¼å¼ï¼š
        "[SYM] ç—‡çŠ¶1 ç—‡çŠ¶2 [SEP] [DESC] æè¿°æ–‡æœ¬ [SEP] [CAUSE] ç—…å› æ–‡æœ¬ [SEP] [CAT] ç§‘å®¤"
    
    ç¤ºä¾‹ï¼š
        >>> format_doc(['å¤´ç—›', 'å‘çƒ­'], 'å¸¸è§æ„Ÿå†’ç—‡çŠ¶', 'ç—…æ¯’æ„ŸæŸ“', 'å‘¼å¸å†…ç§‘')
        '[SYM] å¤´ç—› å‘çƒ­ [SEP] [DESC] å¸¸è§æ„Ÿå†’ç—‡çŠ¶ [SEP] [CAUSE] ç—…æ¯’æ„ŸæŸ“ [SEP] [CAT] å‘¼å¸å†…ç§‘'
    """
    parts = []  # å­˜å‚¨å„ä¸ªå­—æ®µ
    
    # 1. ç—‡çŠ¶å­—æ®µï¼ˆå¿…æœ‰ï¼‰
    symp_str = ' '.join([t for t in symps if t])
    parts.append(f"{FIELD_TAGS[0]} {symp_str}".strip())
    
    # 2. æè¿°å­—æ®µï¼ˆå¯é€‰ï¼‰
    if desc:
        parts.append(f"[SEP] {FIELD_TAGS[1]} {desc}".strip())
    
    # 3. ç—…å› å­—æ®µï¼ˆå¯é€‰ï¼‰
    if cause:
        parts.append(f"[SEP] {FIELD_TAGS[2]} {cause}".strip())
    
    # 4. ç§‘å®¤å­—æ®µï¼ˆå¯é€‰ï¼‰
    if cat:
        parts.append(f"[SEP] {FIELD_TAGS[3]} {cat}".strip())
    
    # 5. ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰å­—æ®µ
    return ' '.join(parts).strip()

def build_rows(df):
    """
    ä»DataFrameæ„å»ºè®­ç»ƒæ ·æœ¬ï¼ˆQuery-Documentå¯¹ï¼‰
    
    å‚æ•°ï¼š
        df (pd.DataFrame): åŒ»ç–—æ•°æ®è¡¨æ ¼ï¼ŒåŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - name: ç–¾ç—…åç§°
            - symptom: ç—‡çŠ¶åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
            - desc: ç–¾ç—…æè¿°
            - cause: ç—…å› 
            - category: ç§‘å®¤åˆ†ç±»ï¼ˆå­—ç¬¦ä¸²å½¢å¼åˆ—è¡¨ï¼‰
    
    è¿”å›ï¼š
        list[dict]: è®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
            - query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆç—‡çŠ¶ï¼‰
            - doc: æ–‡æ¡£æ–‡æœ¬ï¼ˆå¤šå­—æ®µç»“æ„åŒ–ï¼‰
            - name: ç–¾ç—…åç§°
            - category: ç§‘å®¤
            - symptoms: ç—‡çŠ¶åˆ—è¡¨
    
    ç¤ºä¾‹ï¼š
        >>> rows = build_rows(df)
        >>> rows[0]
        {
            'query': '[SYM] å¤´ç—› å‘çƒ­',
            'doc': '[SYM] å¤´ç—› å‘çƒ­ [SEP] [DESC] ...',
            'name': 'æ„Ÿå†’',
            'category': 'å‘¼å¸å†…ç§‘',
            'symptoms': ['å¤´ç—›', 'å‘çƒ­', 'å’³å—½']
        }
    """
    rows = []
    
    # éå†æ¯ä¸€è¡Œç–¾ç—…æ•°æ®
    for _, r in df.iterrows():
        # 1. æå–ç–¾ç—…åç§°
        name = str(r.get('name', '')).strip()
        
        # 2. æå–æè¿°å’Œç—…å› ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        desc = '' if pd.isna(r.get('desc')) else str(r.get('desc'))
        cause = '' if pd.isna(r.get('cause')) else str(r.get('cause'))
        
        # 3. è§£æç—‡çŠ¶åˆ—è¡¨
        symps = parse_list_str(r.get('symptom'))
        
        # 4. è§£æç§‘å®¤åˆ†ç±»ï¼ˆå–æœ€åä¸€çº§ï¼‰
        cats = parse_list_str(r.get('category'))
        cat = cats[-1] if len(cats) > 0 else ''
        
        # 5. æ ¼å¼åŒ–æŸ¥è¯¢æ–‡æœ¬ï¼ˆä»…ç—‡çŠ¶ï¼‰
        q = format_query(symps)
        
        # 6. å¦‚æœæ²¡æœ‰ç—‡çŠ¶ï¼Œè·³è¿‡è¯¥ç–¾ç—…
        if not q:
            continue
        
        # 7. æ ¼å¼åŒ–æ–‡æ¡£æ–‡æœ¬ï¼ˆå¤šå­—æ®µï¼‰
        doc = format_doc(symps, desc, cause, cat)
        
        # 8. æ„å»ºæ ·æœ¬å­—å…¸
        rows.append({
            'query': q,
            'doc': doc,
            'name': name,
            'category': cat,
            'symptoms': symps
        })
    
    return rows

# ==================== ç¬¬4éƒ¨åˆ†ï¼šBERTæ¨¡å‹ç›¸å…³ ====================

def mean_pooling(last_hidden_state, attention_mask):
    """
    å¹³å‡æ± åŒ–ï¼ˆå°†BERTè¾“å‡ºçš„åºåˆ—å‘é‡è½¬ä¸ºå•ä¸ªå‘é‡ï¼‰
    
    å‚æ•°ï¼š
        last_hidden_state (Tensor): BERTæœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ (batch, seq_len, hidden_dim)
        attention_mask (Tensor): æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ (batch, seq_len)ï¼Œ0è¡¨ç¤ºpaddingä½ç½®
    
    è¿”å›ï¼š
        Tensor: æ± åŒ–åçš„å‘é‡ï¼Œå½¢çŠ¶ (batch, hidden_dim)
    
    è®¡ç®—æ–¹æ³•ï¼š
        1. å¯¹épaddingä½ç½®çš„å‘é‡æ±‚å’Œ
        2. é™¤ä»¥æœ‰æ•ˆtokenæ•°é‡ï¼ˆå¿½ç•¥paddingï¼‰
    
    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š[CLS] å¤´ ç—› [SEP] [PAD] [PAD]  (hidden_dim=768)
        è¾“å‡ºï¼š4ä¸ªtokençš„å¹³å‡å‘é‡ï¼ˆå¿½ç•¥[PAD]ï¼‰
    """
    # 1. å°†attention_maskæ‰©å±•ä¸º3Då¼ é‡ (batch, seq_len, 1)
    mask = attention_mask.unsqueeze(-1).float()
    
    # 2. å¯¹æœ‰æ•ˆä½ç½®çš„å‘é‡æ±‚å’Œ (batch, hidden_dim)
    summed = (last_hidden_state * mask).sum(dim=1)
    
    # 3. è®¡ç®—æœ‰æ•ˆtokenæ•°é‡ (batch, 1)
    counts = mask.sum(dim=1).clamp(min=1e-9)  # é˜²æ­¢é™¤ä»¥0
    
    # 4. è¿”å›å¹³å‡å‘é‡ (batch, hidden_dim)
    return summed / counts

class BiEncoder(torch.nn.Module):
    """
    åŒå¡”ç¼–ç å™¨ï¼ˆQueryå’ŒDocumentå…±äº«BERTå‚æ•°ï¼‰
    
    æ¶æ„ï¼š
        Query â†’ BERT â†’ Mean Pooling â†’ L2å½’ä¸€åŒ– â†’ Queryå‘é‡
        Doc   â†’ BERT â†’ Mean Pooling â†’ L2å½’ä¸€åŒ– â†’ Docå‘é‡
    
    è®­ç»ƒç›®æ ‡ï¼š
        æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹(q, d+)çš„ä½™å¼¦ç›¸ä¼¼åº¦
        æœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹(q, d-)çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    def __init__(self, model_path):
        """
        åˆå§‹åŒ–åŒå¡”ç¼–ç å™¨
        
        å‚æ•°ï¼š
            model_path (str): BERTæ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æˆ–HuggingFaceï¼‰
        """
        super().__init__()
        # åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹ï¼ˆä»…ç¼–ç å™¨éƒ¨åˆ†ï¼‰
        self.bert = BertModel.from_pretrained(model_path, local_files_only=True)
    
    def encode(self, input_ids, attention_mask):
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        å‚æ•°ï¼š
            input_ids (Tensor): è¾“å…¥token IDï¼Œå½¢çŠ¶ (batch, seq_len)
            attention_mask (Tensor): æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ (batch, seq_len)
        
        è¿”å›ï¼š
            Tensor: L2å½’ä¸€åŒ–åçš„å‘é‡ï¼Œå½¢çŠ¶ (batch, hidden_dim)
        """
        # 1. BERTå‰å‘ä¼ æ’­
        out = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # 2. å¹³å‡æ± åŒ–
        pooled = mean_pooling(out.last_hidden_state, attention_mask)
        
        # 3. L2å½’ä¸€åŒ–ï¼ˆä½¿ä½™å¼¦ç›¸ä¼¼åº¦ç­‰äºç‚¹ç§¯ï¼‰
        return torch.nn.functional.normalize(pooled, p=2, dim=1)

class PairDataset(Dataset):
    """
    Query-Documentå¯¹æ•°æ®é›†ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
    
    æ•°æ®æ ¼å¼ï¼š
        [
            {'query': '[SYM] å¤´ç—›', 'doc': '[SYM] å¤´ç—› [SEP] [DESC] ...', ...},
            {'query': '[SYM] å‘çƒ­', 'doc': '[SYM] å‘çƒ­ [SEP] [DESC] ...', ...},
            ...
        ]
    """
    def __init__(self, rows):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°ï¼š
            rows (list[dict]): æ ·æœ¬åˆ—è¡¨ï¼ˆç”±build_rowsç”Ÿæˆï¼‰
        """
        self.rows = rows
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.rows)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        å‚æ•°ï¼š
            idx (int): æ ·æœ¬ç´¢å¼•
        
        è¿”å›ï¼š
            dict: åŒ…å« queryã€docã€nameã€categoryã€symptoms çš„å­—å…¸
        """
        return self.rows[idx]

def collate_fn(batch, tokenizer):
    """
    æ‰¹é‡æ•°æ®å¤„ç†å‡½æ•°ï¼ˆå°†å¤šä¸ªæ ·æœ¬åˆå¹¶ä¸ºbatchï¼‰
    
    å‚æ•°ï¼š
        batch (list[dict]): æ‰¹æ¬¡æ ·æœ¬åˆ—è¡¨
        tokenizer (BertTokenizer): BERTåˆ†è¯å™¨
    
    è¿”å›ï¼š
        dict: åŒ…å«ä»¥ä¸‹é”®å€¼å¯¹ï¼š
            - q_input_ids: Queryçš„token ID (batch, seq_len)
            - q_attn_mask: Queryçš„æ³¨æ„åŠ›æ©ç  (batch, seq_len)
            - d_input_ids: Documentçš„token ID (batch, seq_len)
            - d_attn_mask: Documentçš„æ³¨æ„åŠ›æ©ç  (batch, seq_len)
    
    å·¥ä½œæµç¨‹ï¼š
        1. æå–æ‰€æœ‰queryå’Œdocæ–‡æœ¬
        2. ä½¿ç”¨tokenizeræ‰¹é‡ç¼–ç ï¼ˆè‡ªåŠ¨paddingï¼‰
        3. è¿”å›PyTorchå¼ é‡
    """
    # 1. æå–æ–‡æœ¬åˆ—è¡¨
    q_texts = [b['query'] for b in batch]  # ['[SYM] å¤´ç—›', '[SYM] å‘çƒ­', ...]
    d_texts = [b['doc'] for b in batch]    # ['[SYM] å¤´ç—› [SEP] ...', ...]
    
    # 2. æ‰¹é‡ç¼–ç Queryï¼ˆè‡ªåŠ¨paddingåˆ°batchå†…æœ€å¤§é•¿åº¦ï¼‰
    q_enc = tokenizer(
        q_texts,
        max_length=MAX_LEN,  # æœ€å¤§é•¿åº¦128
        truncation=True,     # è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­
        padding=True,        # paddingåˆ°batchå†…æœ€å¤§é•¿åº¦
        return_tensors='pt'  # è¿”å›PyTorchå¼ é‡
    )
    
    # 3. æ‰¹é‡ç¼–ç Document
    d_enc = tokenizer(
        d_texts,
        max_length=MAX_LEN,
        truncation=True,
        padding=True,
        return_tensors='pt'
    )
    
    # 4. è¿”å›batchå­—å…¸
    return {
        'q_input_ids': q_enc['input_ids'],      # Queryçš„token ID
        'q_attn_mask': q_enc['attention_mask'],  # Queryçš„æ³¨æ„åŠ›æ©ç 
        'd_input_ids': d_enc['input_ids'],       # Documentçš„token ID
        'd_attn_mask': d_enc['attention_mask']   # Documentçš„æ³¨æ„åŠ›æ©ç 
    }

def add_field_tags_to_tokenizer_and_model(tokenizer, model_bert):
    """
    æ·»åŠ å­—æ®µæ ‡è®°åˆ°tokenizerå’ŒBERTæ¨¡å‹
    
    å‚æ•°ï¼š
        tokenizer (BertTokenizer): BERTåˆ†è¯å™¨
        model_bert (BertModel): BERTæ¨¡å‹
    
    è¿”å›ï¼š
        int: æ–°å¢tokençš„æ•°é‡
    
    ä½œç”¨ï¼š
        å°† [SYM]ã€[DESC]ã€[CAUSE]ã€[CAT] æ·»åŠ åˆ°è¯è¡¨ä¸­
        å¹¶è°ƒæ•´BERTåµŒå…¥å±‚ç»´åº¦ï¼ˆæ‰©å±•tokenåµŒå…¥çŸ©é˜µï¼‰
    """
    # 1. æ·»åŠ ç‰¹æ®Štokenåˆ°åˆ†è¯å™¨
    num_added = tokenizer.add_special_tokens({
        'additional_special_tokens': FIELD_TAGS  # ['[SYM]', '[DESC]', '[CAUSE]', '[CAT]']
    })
    
    # 2. å¦‚æœæœ‰æ–°å¢tokenï¼Œè°ƒæ•´æ¨¡å‹åµŒå…¥å±‚ç»´åº¦
    if num_added > 0:
        model_bert.resize_token_embeddings(len(tokenizer))
    
    return num_added

@torch.no_grad()  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
def embed_texts_with_bert(bert_model, tokenizer, texts, device, desc='Encode'):
    """
    æ‰¹é‡ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
    
    å‚æ•°ï¼š
        bert_model (BertModel): BERTæ¨¡å‹
        tokenizer (BertTokenizer): åˆ†è¯å™¨
        texts (list[str]): æ–‡æœ¬åˆ—è¡¨
        device (torch.device): è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        desc (str): è¿›åº¦æ¡æè¿°
    
    è¿”å›ï¼š
        np.ndarray: å‘é‡çŸ©é˜µï¼Œå½¢çŠ¶ (len(texts), hidden_dim)
    
    å·¥ä½œæµç¨‹ï¼š
        1. åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
        2. ç¼–ç æ¯æ‰¹æ–‡æœ¬
        3. æ‹¼æ¥æ‰€æœ‰å‘é‡
    """
    vecs = []  # å­˜å‚¨æ‰€æœ‰å‘é‡
    bert_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutï¼‰
    
    # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹BATCH_SIZEä¸ªæ ·æœ¬ï¼‰
    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=desc):
        batch = texts[i:i+BATCH_SIZE]  # è·å–å½“å‰æ‰¹æ¬¡
        
        # ç¼–ç å½“å‰æ‰¹æ¬¡
        enc = tokenizer(
            batch,
            max_length=MAX_LEN,
            truncation=True,
            padding=True,
            return_tensors='pt'
        )
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        enc = {k: v.to(device) for k, v in enc.items()}
        
        # BERTå‰å‘ä¼ æ’­
        out = bert_model(**enc, return_dict=True)
        
        # å¹³å‡æ± åŒ–
        pooled = mean_pooling(out.last_hidden_state, enc['attention_mask'])
        
        # L2å½’ä¸€åŒ–
        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)
        
        # è½¬ä¸ºNumPyå¹¶å­˜å‚¨
        vecs.append(pooled.cpu().numpy())
    
    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„å‘é‡ (N, hidden_dim)
    return np.vstack(vecs)

# ==================== ç¬¬5éƒ¨åˆ†ï¼šè®­ç»ƒå’Œè¯„ä¼° ====================

def grouped_split_by_name(rows, val_ratio=0.1, seed=SEED):
    """
    æŒ‰ç–¾ç—…åç§°åˆ†ç»„åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆé¿å…åŒç—…æ³„æ¼ï¼‰
    
    å‚æ•°ï¼š
        rows (list[dict]): æ ·æœ¬åˆ—è¡¨
        val_ratio (float): éªŒè¯é›†æ¯”ä¾‹
        seed (int): éšæœºç§å­
    
    è¿”å›ï¼š
        tuple: (train_rows, val_rows)
    
    åˆ’åˆ†ç­–ç•¥ï¼š
        1. å°†åŒä¸€ç–¾ç—…çš„æ‰€æœ‰æ ·æœ¬åˆ†ä¸ºä¸€ç»„
        2. æŒ‰ç–¾ç—…åç§°éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        3. ç¡®ä¿åŒä¸€ç–¾ç—…ä¸ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š[
            {'name': 'æ„Ÿå†’', ...},
            {'name': 'æ„Ÿå†’', ...},
            {'name': 'å‘çƒ§', ...}
        ]
        è¾“å‡ºï¼š
            è®­ç»ƒé›†ï¼š[{'name': 'æ„Ÿå†’', ...}, {'name': 'æ„Ÿå†’', ...}]
            éªŒè¯é›†ï¼š[{'name': 'å‘çƒ§', ...}]
    """
    from collections import defaultdict
    
    # 1. æŒ‰ç–¾ç—…åç§°åˆ†ç»„
    groups = defaultdict(list)  # {'æ„Ÿå†’': [0, 1], 'å‘çƒ§': [2], ...}
    for i, r in enumerate(rows):
        groups[r['name']].append(i)
    
    # 2. è·å–æ‰€æœ‰ç–¾ç—…åç§°
    names = list(groups.keys())
    
    # 3. éšæœºæ‰“ä¹±ç–¾ç—…é¡ºåº
    random.Random(seed).shuffle(names)
    
    # 4. è®¡ç®—è®­ç»ƒé›†ç–¾ç—…æ•°é‡
    cut = max(1, int(len(names) * (1 - val_ratio)))
    
    # 5. åˆ’åˆ†ç–¾ç—…åç§°
    train_names = set(names[:cut])  # å‰90%çš„ç–¾ç—…
    
    # 6. æ”¶é›†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ ·æœ¬ç´¢å¼•
    train_idx = [i for n in train_names for i in groups[n]]
    val_idx = [i for n in names[cut:] for i in groups[n]]
    
    # 7. è¿”å›æ ·æœ¬åˆ—è¡¨
    return [rows[i] for i in train_idx], [rows[i] for i in val_idx]

def train(epochs=EPOCHS):
    """
    è®­ç»ƒåŒå¡”ç¼–ç å™¨ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
    
    å‚æ•°ï¼š
        epochs (int): è®­ç»ƒè½®æ•°
    
    è®­ç»ƒæµç¨‹ï¼š
        1. åŠ è½½æ•°æ®å¹¶æ„å»ºQuery-Documentå¯¹
        2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæŒ‰ç–¾ç—…åç§°åˆ†ç»„ï¼‰
        3. åˆå§‹åŒ–BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        4. å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼ˆInfoNCEæŸå¤±ï¼‰
        5. è¯„ä¼°éªŒè¯é›†Recall@10
        6. ä¿å­˜æœ€ä½³æ¨¡å‹
    
    å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼š
        L = -log( exp(qÂ·d+/Ï„) / Î£exp(qÂ·di/Ï„) )
        å…¶ä¸­ï¼š
        - q: queryå‘é‡
        - d+: æ­£æ ·æœ¬documentå‘é‡
        - di: batchå†…æ‰€æœ‰documentå‘é‡ï¼ˆåŒ…æ‹¬è´Ÿæ ·æœ¬ï¼‰
        - Ï„: æ¸©åº¦å‚æ•°ï¼ˆTEMP=0.05ï¼‰
    """
    # ===== ç¬¬1æ­¥ï¼šåŠ è½½æ•°æ® =====
    print('ğŸ“‚ è¯»å–æ•°æ®...')
    df = pd.read_csv(DATA_PATH, encoding='utf-8')
    rows = build_rows(df)  # æ„å»ºè®­ç»ƒæ ·æœ¬
    print(f'âœ“ å¯è®­ç»ƒæ¡ç›®: {len(rows)}')

    if len(rows) < 2:
        raise RuntimeError('æ²¡æœ‰è¶³å¤Ÿçš„æœ‰ç—‡çŠ¶æ¡ç›®ç”¨äºè®­ç»ƒ')

    # ===== ç¬¬2æ­¥ï¼šåˆ’åˆ†æ•°æ®é›† =====
    # ä½¿ç”¨åˆ†ç»„åˆ’åˆ†ï¼ˆé¿å…åŒä¸€ç–¾ç—…åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼‰
    train_rows, val_rows = grouped_split_by_name(rows, val_ratio=0.1, seed=SEED)

    # ===== ç¬¬3æ­¥ï¼šåˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ =====
    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
    model = BiEncoder(MODEL_PATH)
    
    # æ·»åŠ å­—æ®µæ ‡è®°åˆ°åˆ†è¯å™¨å’Œæ¨¡å‹
    add_field_tags_to_tokenizer_and_model(tokenizer, model.bert)

    # ===== ç¬¬4æ­¥ï¼šè®¾ç½®è®¾å¤‡ =====
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    print(f'âœ“ è®¾å¤‡: {device}')

    # ===== ç¬¬5æ­¥ï¼šåˆ›å»ºæ•°æ®åŠ è½½å™¨ =====
    train_ds = PairDataset(train_rows)
    val_ds = PairDataset(val_rows)
    
    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,  # è®­ç»ƒé›†æ‰“ä¹±
        collate_fn=lambda b: collate_fn(b, tokenizer)
    )
    
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE,
        shuffle=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
        collate_fn=lambda b: collate_fn(b, tokenizer)
    )

    # ===== ç¬¬6æ­¥ï¼šåˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ =====
    optimizer = AdamW(
        model.parameters(),
        lr=LR,  # å­¦ä¹ ç‡2e-5
        weight_decay=0.01  # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
    )
    
    total_steps = max(1, len(train_loader) * epochs)  # æ€»è®­ç»ƒæ­¥æ•°
    warmup_steps = int(total_steps * WARMUP)  # é¢„çƒ­æ­¥æ•°ï¼ˆ10%ï¼‰
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,  # é¢„çƒ­é˜¶æ®µå­¦ä¹ ç‡çº¿æ€§å¢é•¿
        num_training_steps=total_steps  # è®­ç»ƒé˜¶æ®µå­¦ä¹ ç‡çº¿æ€§è¡°å‡
    )
    
    ce = torch.nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰

    # ===== ç¬¬7æ­¥ï¼šè®­ç»ƒå¾ªç¯ =====
    best_r10 = 0.0  # è®°å½•æœ€ä½³Recall@10
    
    for epoch in range(epochs):
        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutï¼‰
        pbar = tqdm(train_loader, desc=f'ğŸ”§ è®­ç»ƒ {epoch+1}/{epochs}')
        loss_running = 0.0  # ç´¯è®¡æŸå¤±
        seen = 0  # å·²å¤„ç†æ ·æœ¬æ•°
        
        for batch in pbar:
            # 1. è·å–batchæ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            q_ids = batch['q_input_ids'].to(device)
            q_ms = batch['q_attn_mask'].to(device)
            d_ids = batch['d_input_ids'].to(device)
            d_ms = batch['d_attn_mask'].to(device)

            # 2. ç¼–ç queryå’Œdocument
            q_vec = model.encode(q_ids, q_ms)  # (batch, hidden_dim)
            d_vec = model.encode(d_ids, d_ms)  # (batch, hidden_dim)

            # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆquery-documentï¼‰
            logits_qd = (q_vec @ d_vec.t()) / TEMP  # (batch, batch)
            # å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
            # éå¯¹è§’çº¿å…ƒç´ æ˜¯è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
            
            # 4. æ„é€ æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼‰
            labels = torch.arange(logits_qd.size(0), device=device)
            # labels = [0, 1, 2, ...] è¡¨ç¤ºç¬¬iä¸ªqueryå¯¹åº”ç¬¬iä¸ªdocument
            
            # 5. è®¡ç®—queryâ†’documentçš„å¯¹æ¯”æŸå¤±
            loss1 = ce(logits_qd, labels)

            # 6. è®¡ç®—documentâ†’queryçš„å¯¹æ¯”æŸå¤±ï¼ˆå¯¹ç§°æŸå¤±ï¼‰
            logits_dq = (d_vec @ q_vec.t()) / TEMP  # (batch, batch)
            loss2 = ce(logits_dq, labels)

            # 7. æœ€ç»ˆæŸå¤±ï¼ˆåŒå‘å¯¹æ¯”æŸå¤±çš„å¹³å‡ï¼‰
            loss = 0.5 * (loss1 + loss2)

            # 8. åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            loss.backward()  # åå‘ä¼ æ’­
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            optimizer.step()  # æ›´æ–°å‚æ•°
            scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

            # 9. è®°å½•æŸå¤±
            loss_running += loss.item() * q_ids.size(0)
            seen += q_ids.size(0)
            pbar.set_postfix({'loss': f'{loss_running / max(1, seen):.4f}'})

        # ===== éªŒè¯é˜¶æ®µ =====
        r10 = eval_recall(model, tokenizer, val_rows, device, topk=10)
        print(f'ğŸ“Š éªŒè¯é›† Recall@10: {r10:.4f}')
        
        # ===== ä¿å­˜æœ€ä½³æ¨¡å‹ =====
        if r10 > best_r10:
            best_r10 = r10
            save_path = os.path.join(OUT_DIR, 'biencoder')
            model.bert.save_pretrained(save_path)  # ä¿å­˜BERTå‚æ•°
            tokenizer.save_pretrained(save_path)  # ä¿å­˜åˆ†è¯å™¨
            print(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {save_path} (R@10={best_r10:.4f})')

    print(f'ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³ R@10={best_r10:.4f}')

@torch.no_grad()  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
def eval_recall(model, tokenizer, rows, device, topk=10, max_eval=512):
    """
    è¯„ä¼°éªŒè¯é›†çš„Recall@Kï¼ˆå¬å›ç‡ï¼‰
    
    å‚æ•°ï¼š
        model (BiEncoder): åŒå¡”ç¼–ç å™¨
        tokenizer (BertTokenizer): åˆ†è¯å™¨
        rows (list[dict]): éªŒè¯é›†æ ·æœ¬
        device (torch.device): è®¾å¤‡
        topk (int): è®¡ç®—Recall@K
        max_eval (int): æœ€å¤šè¯„ä¼°çš„æ ·æœ¬æ•°ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
    
    è¿”å›ï¼š
        float: Recall@Kï¼ˆèŒƒå›´0~1ï¼‰
    
    è¯„ä¼°æ–¹æ³•ï¼š
        1. ç¼–ç æ‰€æœ‰queryå’Œdocument
        2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (N, N)
        3. å¯¹æ¯ä¸ªqueryï¼Œæ‰¾åˆ°Top-Kæœ€ç›¸ä¼¼çš„document
        4. å¦‚æœæ­£æ ·æœ¬åœ¨Top-Kä¸­ï¼Œè®¡ä¸ºå‘½ä¸­
        5. Recall@K = å‘½ä¸­æ•° / æ€»æ ·æœ¬æ•°
    """
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    # 1. é‡‡æ ·ï¼ˆå¦‚æœæ ·æœ¬å¤ªå¤šï¼Œåªè¯„ä¼°å‰max_evalä¸ªï¼‰
    sample = rows[:max_eval] if len(rows) > max_eval else rows
    
    # 2. æå–queryå’Œdocumentæ–‡æœ¬
    q_texts = [r['query'] for r in sample]
    d_texts = [r['doc'] for r in sample]

    # 3. ç¼–ç ä¸ºå‘é‡
    q_vecs = embed_texts_with_bert(model.bert, tokenizer, q_texts, device, desc='è¯„ä¼°æŸ¥è¯¢')
    d_vecs = embed_texts_with_bert(model.bert, tokenizer, d_texts, device, desc='è¯„ä¼°æ–‡æ¡£')

    # 4. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (N, N)
    sims = q_vecs @ d_vecs.T
    
    # 5. å¯¹æ¯è¡Œæ’åºï¼Œæ‰¾åˆ°Top-K document
    idxs = np.argsort(-sims, axis=1)[:, :topk]  # (N, topk)
    
    # 6. è®¡ç®—å‘½ä¸­æ•°
    hits = 0
    for i in range(idxs.shape[0]):
        if i in idxs[i]:  # å¦‚æœæ­£æ ·æœ¬ï¼ˆç¬¬iä¸ªdocumentï¼‰åœ¨Top-Kä¸­
            hits += 1
    
    # 7. è¿”å›å¬å›ç‡
    return hits / idxs.shape[0]

# ==================== ç¬¬6éƒ¨åˆ†ï¼šæ„å»ºç´¢å¼• ====================

@torch.no_grad()
def build_index():
    """
    æ„å»ºç–¾ç—…ç´¢å¼•ï¼ˆåŒ…å«è¯­ä¹‰å‘é‡å’ŒIDFæƒé‡ï¼‰
    
    ç´¢å¼•å†…å®¹ï¼š
        - embeddings: æ‰€æœ‰ç–¾ç—…çš„BERTå‘é‡ (N, hidden_dim)
        - names: ç–¾ç—…åç§°åˆ—è¡¨ (N,)
        - categories: ç§‘å®¤åˆ—è¡¨ (N,)
        - symptoms: ç—‡çŠ¶åˆ—è¡¨ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰ (N,)
        - docs: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨ (N,)
        - idf_terms: IDFè¯è¡¨ï¼ˆç—‡çŠ¶è¯ï¼‰
        - idf_vals: IDFå€¼
    
    å·¥ä½œæµç¨‹ï¼š
        1. åŠ è½½è®­ç»ƒåçš„BERTæ¨¡å‹
        2. è¯»å–åŒ»ç–—æ•°æ®
        3. ç¼–ç æ‰€æœ‰ç–¾ç—…æ–‡æ¡£
        4. è®¡ç®—ç—‡çŠ¶IDFæƒé‡
        5. ä¿å­˜ä¸ºå‹ç¼©çš„NumPyæ–‡ä»¶
    """
    # ===== ç¬¬1æ­¥ï¼šåŠ è½½è®­ç»ƒåçš„æ¨¡å‹ =====
    print('ğŸ“¦ è½½å…¥è®­ç»ƒåçš„ç¼–ç å™¨...')
    enc_path = os.path.join(OUT_DIR, 'biencoder')
    
    if not os.path.isdir(enc_path):
        raise FileNotFoundError(f'æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹: {enc_path}ã€‚è¯·å…ˆè¿è¡Œ --train')
    
    tok = BertTokenizer.from_pretrained(enc_path, local_files_only=True)
    enc = BertModel.from_pretrained(enc_path, local_files_only=True)

    # æ·»åŠ å­—æ®µæ ‡è®°
    tok.add_special_tokens({'additional_special_tokens': FIELD_TAGS})
    enc.resize_token_embeddings(len(tok))

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    enc.to(device).eval()

    # ===== ç¬¬2æ­¥ï¼šè¯»å–æ•°æ® =====
    print('ğŸ“‚ è¯»å–æ•°æ®å¹¶æ„å»ºæ–‡æ¡£...')
    df = pd.read_csv(DATA_PATH, encoding='utf-8')
    rows = build_rows(df)
    
    names = [r['name'] for r in rows]  # ç–¾ç—…åç§°åˆ—è¡¨
    cats = [r['category'] for r in rows]  # ç§‘å®¤åˆ—è¡¨
    symps = [r['symptoms'] for r in rows]  # ç—‡çŠ¶åˆ—è¡¨
    docs = [r['doc'] for r in rows]  # æ–‡æ¡£åˆ—è¡¨

    # ===== ç¬¬3æ­¥ï¼šè®¡ç®—ç—‡çŠ¶IDFæƒé‡ =====
    # IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰= log((N+1) / (DF+1)) + 1
    # å…¶ä¸­ï¼š
    # - N: æ€»ç–¾ç—…æ•°
    # - DF: åŒ…å«è¯¥ç—‡çŠ¶çš„ç–¾ç—…æ•°
    # 
    # ä½œç”¨ï¼š
    # - å¸¸è§ç—‡çŠ¶ï¼ˆå¦‚"å‘çƒ­"ï¼‰IDFè¾ƒä½ï¼Œæƒé‡å°
    # - ç½•è§ç—‡çŠ¶ï¼ˆå¦‚"ç‰™é¾ˆå‡ºè¡€"ï¼‰IDFè¾ƒé«˜ï¼Œæƒé‡å¤§
    
    N = len(rows)  # æ€»ç–¾ç—…æ•°
    df_counts = {}  # ç»Ÿè®¡æ¯ä¸ªç—‡çŠ¶çš„æ–‡æ¡£é¢‘ç‡
    
    # ç»Ÿè®¡æ–‡æ¡£é¢‘ç‡
    for r in rows:
        for s in set([t for t in r['symptoms'] if t]):
            df_counts[s] = df_counts.get(s, 0) + 1
    
    # è®¡ç®—IDF
    idf_terms = []  # ç—‡çŠ¶è¯è¡¨
    idf_vals = []  # IDFå€¼
    
    for s, dfc in df_counts.items():
        idf_terms.append(s)
        # IDFå…¬å¼ï¼šlog((N+1)/(DF+1)) + 1
        # +1å¹³æ»‘é¿å…log(0)ï¼Œ+1åç½®ç¡®ä¿éè´Ÿ
        idf_vals.append(log((N + 1.0) / (dfc + 1.0)) + 1.0)
    
    idf_terms = np.array(idf_terms, dtype=object)
    idf_vals = np.array(idf_vals, dtype=np.float32)

    # ===== ç¬¬4æ­¥ï¼šç¼–ç æ‰€æœ‰æ–‡æ¡£ =====
    print(f'ğŸ¤– ç¼–ç  {len(docs)} ä¸ªæ–‡æ¡£å‘é‡...')
    vecs = embed_texts_with_bert(enc, tok, docs, device, desc='ç¼–ç æ–‡æ¡£')

    # ===== ç¬¬5æ­¥ï¼šä¿å­˜ç´¢å¼• =====
    np.savez_compressed(
        INDEX_PATH,
        embeddings=vecs,  # (N, hidden_dim)
        names=np.array(names, dtype=object),
        categories=np.array(cats, dtype=object),
        symptoms=np.array([json.dumps(s, ensure_ascii=False) for s in symps], dtype=object),
        docs=np.array(docs, dtype=object),
        idf_terms=idf_terms,  # ç—‡çŠ¶è¯è¡¨
        idf_vals=idf_vals  # IDFå€¼
    )
    
    print(f'âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {INDEX_PATH}')

# ==================== ç¬¬7éƒ¨åˆ†ï¼šæ£€ç´¢å’Œè¯é¢åŒ¹é… ====================

def _tokens_from_query_text(text):
    """
    ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–ç—‡çŠ¶è¯ï¼ˆå»é™¤å­—æ®µæ ‡è®°ï¼‰
    
    å‚æ•°ï¼š
        text (str): æŸ¥è¯¢æ–‡æœ¬ï¼Œå¦‚ "[SYM] å¤´ç—› å‘çƒ­"
    
    è¿”å›ï¼š
        list[str]: ç—‡çŠ¶è¯åˆ—è¡¨ï¼Œå¦‚ ['å¤´ç—›', 'å‘çƒ­']
    
    ç¤ºä¾‹ï¼š
        >>> _tokens_from_query_text("[SYM] å¤´ç—› å‘çƒ­ [SEP] [DESC]")
        ['å¤´ç—›', 'å‘çƒ­']
    """
    toks = []
    for t in text.strip().split():
        # è·³è¿‡å­—æ®µæ ‡è®°å’Œåˆ†éš”ç¬¦
        if t in FIELD_TAGS or t == '[SEP]':
            continue
        toks.append(t)
    return toks

def _char_ngrams(s, n=2):
    """
    ç”Ÿæˆå­—ç¬¦çº§n-gramé›†åˆï¼ˆç”¨äºæ¨¡ç³ŠåŒ¹é…ï¼‰
    
    å‚æ•°ï¼š
        s (str): è¾“å…¥å­—ç¬¦ä¸²
        n (int): n-gramé•¿åº¦ï¼ˆé»˜è®¤2ï¼‰
    
    è¿”å›ï¼š
        set[str]: n-gramé›†åˆ
    
    ç¤ºä¾‹ï¼š
        >>> _char_ngrams("å¤´ç—›", 2)
        {'å¤´ç—›'}
        
        >>> _char_ngrams("åå¤´ç—›", 2)
        {'åå¤´', 'å¤´ç—›'}
    """
    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
    s = str(s).replace('ã€', '').replace('ï¼Œ', '').replace('ã€‚', '').replace(' ', '')
    
    if not s:
        return set()
    
    # å¦‚æœå­—ç¬¦ä¸²é•¿åº¦å°äºnï¼Œè¿”å›æ•´ä¸ªå­—ç¬¦ä¸²
    if len(s) < n:
        return {s}
    
    # ç”Ÿæˆæ»‘åŠ¨çª—å£n-gram
    return {s[i:i+n] for i in range(len(s) - n + 1)}

def _lexical_score(query_tokens, doc_symptoms, mode='fuzzy', idf=None):
    """
    è®¡ç®—è¯é¢åŒ¹é…å¾—åˆ†ï¼ˆæ”¯æŒå¤šç§æ¨¡å¼ï¼‰
    
    å‚æ•°ï¼š
        query_tokens (list[str]): æŸ¥è¯¢ç—‡çŠ¶è¯
        doc_symptoms (list[str]): æ–‡æ¡£ç—‡çŠ¶è¯
        mode (str): åŒ¹é…æ¨¡å¼
            - 'none': ç¦ç”¨è¯é¢åŒ¹é…
            - 'exact': ç²¾ç¡®åŒ¹é…ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
            - 'wexact': åŠ æƒç²¾ç¡®åŒ¹é…ï¼ˆIDFåŠ æƒJaccardï¼‰
            - 'fuzzy': æ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦çº§2-gram Jaccardï¼‰
        idf (dict): IDFå­—å…¸ {è¯: IDFå€¼}
    
    è¿”å›ï¼š
        float: è¯é¢åŒ¹é…å¾—åˆ†ï¼ˆèŒƒå›´0~1ï¼‰
    
    æ¨¡å¼è¯´æ˜ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼ˆexactï¼‰ï¼š
           Jaccard = |Q âˆ© D| / |Q âˆª D|
           ç¤ºä¾‹ï¼šQ=['å¤´ç—›', 'å‘çƒ­'], D=['å¤´ç—›', 'å’³å—½']
                 Jaccard = 1/3 = 0.333
        
        2. åŠ æƒç²¾ç¡®åŒ¹é…ï¼ˆwexactï¼‰ï¼š
           Jaccard = Î£_i IDF(word_i) for word_i in Qâˆ©D / Î£_i IDF(word_i) for word_i in QâˆªD
           ç¤ºä¾‹ï¼šQ=['å¤´ç—›', 'å‘çƒ­'], D=['å¤´ç—›', 'å’³å—½']
                 IDF('å¤´ç—›')=1.5, IDF('å‘çƒ­')=2.0, IDF('å’³å—½')=2.5
                 Jaccard = 1.5 / (1.5+2.0+2.5) = 0.25
        
        3. æ¨¡ç³ŠåŒ¹é…ï¼ˆfuzzyï¼‰ï¼š
           å°†æ¯ä¸ªè¯è½¬ä¸ºå­—ç¬¦çº§2-gramï¼Œç„¶åè®¡ç®—Jaccard
           ç¤ºä¾‹ï¼šQ=['åå¤´ç—›'], D=['å¤´ç—›']
                 Q_ngrams={'åå¤´', 'å¤´ç—›'}, D_ngrams={'å¤´ç—›'}
                 Jaccard = 1/2 = 0.5
    """
    # 1. è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    q_tokens = [t for t in query_tokens if t]
    d_tokens = [t for t in doc_symptoms if t]
    
    if not q_tokens or not d_tokens:
        return 0.0

    # 2. ç¦ç”¨è¯é¢åŒ¹é…
    if mode == 'none':
        return 0.0

    # 3. ç²¾ç¡®åŒ¹é…ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
    if mode == 'exact':
        q_set, d_set = set(q_tokens), set(d_tokens)
        inter, union = len(q_set & d_set), len(q_set | d_set)
        return (inter / union) if union > 0 else 0.0

    # 4. åŠ æƒç²¾ç¡®åŒ¹é…ï¼ˆIDFåŠ æƒJaccardï¼‰
    if mode == 'wexact':
        q_set, d_set = set(q_tokens), set(d_tokens)
        inter = q_set & d_set  # äº¤é›†
        union = q_set | d_set  # å¹¶é›†
        
        if not union:
            return 0.0
        
        def wsum(ts):
            """è®¡ç®—è¯é›†åˆçš„IDFåŠ æƒå’Œ"""
            if not idf:
                return float(len(ts))  # æ²¡æœ‰IDFæ—¶ï¼Œä½¿ç”¨è¯æ•°
            return sum(max(0.0, float(idf.get(t, 0.0))) for t in ts)
        
        # åŠ æƒJaccard
        return (wsum(inter) / max(1e-9, wsum(union)))

    # 5. æ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦çº§2-gram Jaccardï¼‰
    # åœºæ™¯ï¼šæŸ¥è¯¢"åå¤´ç—›"ï¼Œæ–‡æ¡£æœ‰"å¤´ç—›"ï¼Œåº”è¯¥éƒ¨åˆ†åŒ¹é…
    q_ngrams = set()
    for t in q_tokens:
        q_ngrams |= _char_ngrams(t, 2)  # åˆå¹¶æ‰€æœ‰è¯çš„2-gram
    
    d_ngrams = set()
    for t in d_tokens:
        d_ngrams |= _char_ngrams(t, 2)
    
    if not q_ngrams or not d_ngrams:
        return 0.0
    
    inter = len(q_ngrams & d_ngrams)
    union = len(q_ngrams | d_ngrams)
    return (inter / union) if union > 0 else 0.0

# ==================== ç¬¬8éƒ¨åˆ†ï¼šæ£€ç´¢æ¥å£ ====================

@torch.no_grad()
def search(query, topk=5, min_score=0.0, alpha=0.7, lexical='fuzzy', debug=False):
    """
    æ··åˆæ£€ç´¢ï¼ˆBERTè¯­ä¹‰ç›¸ä¼¼åº¦ + è¯é¢åŒ¹é…ï¼‰
    
    å‚æ•°ï¼š
        query (str): æŸ¥è¯¢æ–‡æœ¬ï¼ˆç—‡çŠ¶ï¼Œç©ºæ ¼åˆ†éš”ï¼‰
        topk (int): è¿”å›å‰Kä¸ªç»“æœ
        min_score (float): æœ€ä½å¾—åˆ†é˜ˆå€¼
        alpha (float): è¯­ä¹‰æƒé‡ï¼ˆæœ€ç»ˆåˆ† = alpha*è¯­ä¹‰ + (1-alpha)*è¯é¢ï¼‰
        lexical (str): è¯é¢åŒ¹é…æ¨¡å¼ï¼ˆfuzzy/exact/wexact/noneï¼‰
        debug (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    
    è¿”å›ï¼š
        Noneï¼ˆç›´æ¥æ‰“å°ç»“æœï¼‰
    
    æ£€ç´¢æµç¨‹ï¼š
        1. åŠ è½½ç´¢å¼•å’Œæ¨¡å‹
        2. ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        3. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        4. è®¡ç®—è¯é¢åŒ¹é…å¾—åˆ†
        5. èåˆä¸¤ç§å¾—åˆ†
        6. æ’åºå¹¶è¿”å›Top-K
    
    ç¤ºä¾‹ï¼š
        >>> search("å¤´ç—› å‘çƒ­", topk=5, alpha=0.7, lexical='fuzzy')
        ğŸ” æŸ¥è¯¢: å¤´ç—› å‘çƒ­
         1. æ„Ÿå†’  [å‘¼å¸å†…ç§‘]  ç›¸ä¼¼åº¦: 0.7234
            ç—‡çŠ¶: å¤´ç—›ã€å‘çƒ­ã€å’³å—½
         2. æµæ„Ÿ  [å‘¼å¸å†…ç§‘]  ç›¸ä¼¼åº¦: 0.6891
            ç—‡çŠ¶: å‘çƒ­ã€å¤´ç—›ã€å…¨èº«é…¸ç—›
    """
    # ===== ç¬¬1æ­¥ï¼šåŠ è½½ç´¢å¼• =====
    if not os.path.exists(INDEX_PATH):
        raise FileNotFoundError('ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ --build_index')
    
    data = np.load(INDEX_PATH, allow_pickle=True)
    embs = data['embeddings']  # (N, hidden_dim)
    names = data['names']  # (N,)
    cats = data['categories']  # (N,)
    symps = [json.loads(s) for s in data['symptoms']]  # list[list[str]]
    
    # è¯»å–IDFå­—å…¸
    idf = {}
    if 'idf_terms' in data and 'idf_vals' in data:
        terms = data['idf_terms']
        vals = data['idf_vals']
        for k, v in zip(terms, vals):
            idf[str(k)] = float(v)

    # ===== ç¬¬2æ­¥ï¼šåŠ è½½æ¨¡å‹ =====
    enc_path = os.path.join(OUT_DIR, 'biencoder')
    tok = BertTokenizer.from_pretrained(enc_path, local_files_only=True)
    enc = BertModel.from_pretrained(enc_path, local_files_only=True)
    
    # æ·»åŠ å­—æ®µæ ‡è®°
    tok.add_special_tokens({'additional_special_tokens': FIELD_TAGS})
    enc.resize_token_embeddings(len(tok))

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    enc.to(device).eval()

    # ===== ç¬¬3æ­¥ï¼šé¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬ =====
    # æå–ç—‡çŠ¶è¯
    raw_query_tokens = [t for t in query.strip().split() if t]
    
    # å¦‚æœæŸ¥è¯¢æ–‡æœ¬å·²åŒ…å«å­—æ®µæ ‡è®°ï¼Œä¿æŒåŸæ ·ï¼›å¦åˆ™æ·»åŠ [SYM]æ ‡è®°
    wrapped_query = query if any(t in query for t in FIELD_TAGS) else format_query(raw_query_tokens)

    # ===== ç¬¬4æ­¥ï¼šç¼–ç æŸ¥è¯¢ =====
    enc_in = tok(
        [wrapped_query],
        max_length=MAX_LEN,
        truncation=True,
        padding=True,
        return_tensors='pt'
    )
    enc_in = {k: v.to(device) for k, v in enc_in.items()}
    
    # BERTå‰å‘ä¼ æ’­
    out = enc(**enc_in, return_dict=True)
    
    # å¹³å‡æ± åŒ– + L2å½’ä¸€åŒ–
    q = mean_pooling(out.last_hidden_state, enc_in['attention_mask'])
    q = torch.nn.functional.normalize(q, p=2, dim=1).cpu().numpy()[0]

    # ===== ç¬¬5æ­¥ï¼šè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ =====
    sims = embs @ q  # (N,) ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå› ä¸ºå‘é‡å·²L2å½’ä¸€åŒ–ï¼‰

    # ===== ç¬¬6æ­¥ï¼šè®¡ç®—è¯é¢åŒ¹é…å¾—åˆ† =====
    q_tokens = _tokens_from_query_text(wrapped_query)  # æå–ç—‡çŠ¶è¯
    lex_scores = np.zeros_like(sims)  # (N,)
    
    for i, s in enumerate(symps):
        lex_scores[i] = _lexical_score(q_tokens, s, mode=lexical, idf=idf)

    # ===== ç¬¬7æ­¥ï¼šèåˆä¸¤ç§å¾—åˆ† =====
    # æœ€ç»ˆåˆ† = alpha * è¯­ä¹‰ç›¸ä¼¼åº¦ + (1-alpha) * è¯é¢åŒ¹é…åˆ†
    final_scores = alpha * sims + (1.0 - alpha) * lex_scores

    # ===== ç¬¬8æ­¥ï¼šæ’åºå¹¶è¿”å›Top-K =====
    order = np.argsort(-final_scores)  # é™åºæ’åº

    # ===== ç¬¬9æ­¥ï¼šæ‰“å°ç»“æœ =====
    print(f'\nğŸ” æŸ¥è¯¢: {" ".join(raw_query_tokens) if raw_query_tokens else query}')
    shown = 0
    
    for idx in order:
        # è¿‡æ»¤ä½åˆ†ç»“æœ
        if final_scores[idx] < min_score:
            continue
        
        # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°è¯¦ç»†å¾—åˆ†
        if debug:
            print(f'{shown+1:>2}. {names[idx]} [{cats[idx]}] '
                  f'final={final_scores[idx]:.4f} cos={sims[idx]:.4f} lex={lex_scores[idx]:.4f}')
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šä»…æ‰“å°æœ€ç»ˆå¾—åˆ†
            print(f'{shown+1:>2}. {names[idx]}  [{cats[idx]}]  ç›¸ä¼¼åº¦: {final_scores[idx]:.4f}')
        
        # æ‰“å°ç—‡çŠ¶åˆ—è¡¨ï¼ˆæœ€å¤š8ä¸ªï¼‰
        if len(symps[idx]) > 0:
            print(f'    ç—‡çŠ¶: {"ã€".join(symps[idx][:8])}')
        
        shown += 1
        
        # è¾¾åˆ°Top-Kååœæ­¢
        if shown >= topk:
            break
    
    # æ— ç»“æœæç¤º
    if shown == 0:
        print('âš ï¸  æœªæ‰¾åˆ°è¾¾åˆ°é˜ˆå€¼çš„ç»“æœ')
        print('ğŸ’¡ æç¤ºï¼šè°ƒæ•´ --min_score æˆ–è®¾ç½® --lexical none ä»¥ä»…ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦')

# ==================== ç¬¬9éƒ¨åˆ†ï¼šå‘½ä»¤è¡Œæ¥å£ ====================

def main():
    """
    å‘½ä»¤è¡Œæ¥å£
    
    ç”¨æ³•ç¤ºä¾‹ï¼š
        # è®­ç»ƒæ¨¡å‹
        python script.py --train --epochs 3
        
        # æ„å»ºç´¢å¼•
        python script.py --build_index
        
        # æ£€ç´¢ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        python script.py --query "å¤´ç—› å‘çƒ­" --lexical exact --alpha 0.5
        
        # æ£€ç´¢ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        python script.py --query "åå¤´ç—›" --lexical fuzzy --alpha 0.7
        
        # è°ƒè¯•æ¨¡å¼
        python script.py --query "å¤´ç—›" --debug
    """
    ap = argparse.ArgumentParser(description='åŒ»ç–—ç–¾ç—…é¢„æµ‹ç³»ç»Ÿ - BERTåŒå¡”ç¼–ç å™¨')
    
    # ===== æ¨¡å¼å‚æ•° =====
    ap.add_argument('--train', action='store_true',
                    help='è®­ç»ƒåŒå¡”ç¼–ç å™¨ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰')
    ap.add_argument('--build_index', action='store_true',
                    help='ç”¨è®­ç»ƒåçš„ç¼–ç å™¨é‡å»ºç´¢å¼•ï¼ˆå«IDFï¼‰')
    ap.add_argument('--query', type=str,
                    help='ç—‡çŠ¶æŸ¥è¯¢æ–‡æœ¬ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼Œæ”¯æŒå·²å¸¦å­—æ®µæ ‡è®°çš„è¾“å…¥')
    
    # ===== è®­ç»ƒå‚æ•° =====
    ap.add_argument('--epochs', type=int, default=EPOCHS,
                    help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤1ï¼‰')
    
    # ===== æ£€ç´¢å‚æ•° =====
    ap.add_argument('--topk', type=int, default=5,
                    help='è¿”å›å‰Kä¸ªç»“æœï¼ˆé»˜è®¤5ï¼‰')
    ap.add_argument('--min_score', type=float, default=0.0,
                    help='æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½œç”¨äºé‡æ’ååˆ†æ•°ï¼ˆé»˜è®¤0.0ï¼‰')
    ap.add_argument('--alpha', type=float, default=0.7,
                    help='è¯­ä¹‰æƒé‡ï¼Œæœ€ç»ˆåˆ†=alpha*cos + (1-alpha)*lexicalï¼ˆé»˜è®¤0.7ï¼‰')
    ap.add_argument('--lexical', type=str, default='fuzzy',
                    choices=['fuzzy', 'exact', 'wexact', 'none'],
                    help='è¯é¢é‡å æ¨¡å¼ï¼šfuzzy(æ¨¡ç³Š), exact(ç²¾ç¡®), wexact(IDFåŠ æƒ), none(ç¦ç”¨)')
    ap.add_argument('--debug', action='store_true',
                    help='æ‰“å° cos/lex/final è°ƒè¯•ä¿¡æ¯')
    
    args = ap.parse_args()

    # ===== æ‰§è¡Œå¯¹åº”æ“ä½œ =====
    if args.train:
        train(epochs=args.epochs)
    
    if args.build_index:
        build_index()
    
    if args.query:
        search(
            args.query,
            args.topk,
            args.min_score,
            args.alpha,
            args.lexical,
            args.debug
        )
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•å‚æ•°ï¼Œæ‰“å°å¸®åŠ©ä¿¡æ¯
    if not (args.train or args.build_index or args.query):
        ap.print_help()

# ==================== ç¬¬10éƒ¨åˆ†ï¼šç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    main()
